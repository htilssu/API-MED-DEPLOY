import cv2
import os
import numpy as np
import torch
import faiss
import json
from pathlib import Path
from google.cloud import storage
from transformers import CLIPProcessor, CLIPModel
from dotenv import load_dotenv
import google.generativeai as genai
from transformers import pipeline
from huggingface_hub import login
import timm
from PIL import Image
from torchvision import transforms
import logging
# from transformers import Blip2Processor, Blip2ForConditionalGeneration
import textwrap
import re
from sklearn.metrics.pairwise import cosine_similarity
import requests
import xml.etree.ElementTree as ET
from typing import Optional, List, Dict
from io import BytesIO
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from app.redis_client import redis_client, save_result_to_redis,get_result_by_key
import hashlib
from app.db.mongo import db

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI()

load_dotenv()
login(token=os.getenv("HUGGINGFACE_TOKEN"))

GCS_BUCKET = "kltn-2025"
GCS_IMAGE_PATH = "uploaded_images/"
GCS_KEY_PATH = storage.Client.from_service_account_json("app/iamkey.json")
VECTOR_FILE = "static/processed/embedded_vectors.json"
GCS_FOLDER = "handle_data"
GCS_DATASET = f"dataset"
GCS_DATASET_PATH = f"{GCS_DATASET}/dataset.json"
GCS_INDEX_PATH = f"{GCS_FOLDER}/faiss_index.bin"
GCS_LABELS_PATH = f"{GCS_FOLDER}/labels.npy"
GCS_TEXT_INDEX_PATH = f"{GCS_FOLDER}/faiss_text_index.bin"
GCS_TEXT_LABELS_PATH = f"{GCS_FOLDER}/text_labels.npy"
GCS_ANOMALY_INDEX_PATH = f"{GCS_FOLDER}/faiss_index_anomaly.bin"
GCS_ANOMALY_LABELS_PATH = f"{GCS_FOLDER}/labels_anomaly.npy"
LOCAL_INDEX_PATH = "app/static/faiss/faiss_index.bin"
LOCAL_LABELS_PATH = "app/static/labels/labels.npy"
LOCAL_TEXT_INDEX_PATH = "app/static/faiss/faiss_text_index.bin"
LOCAL_TEXT_LABELS_PATH = "app/static/labels/text_labels.npy"
LOCAL_ANOMALY_INDEX_PATH = "app/static/faiss/faiss_index_anomaly.bin"
LOCAL_ANOMALY_LABELS_PATH = "app/static/labels/labels_anomaly.npy"
LOCAL_DATASET_PATH = "app/static/json/dataset.json"
INDEX_DIM = 512

index = None
labels = []
anomaly_index = None
anomaly_labels = []

device = "cuda" if torch.cuda.is_available() else "cpu"
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
vit_model = timm.create_model("vit_base_patch16_224", pretrained=True).to(device)
vit_model.eval()
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

users_collection = db["users"]

def download_from_gcs():
    storage_client = GCS_KEY_PATH
    bucket = storage_client.bucket(GCS_BUCKET)
    files_to_download = [
        (GCS_INDEX_PATH, LOCAL_INDEX_PATH),
        (GCS_LABELS_PATH, LOCAL_LABELS_PATH),
        (GCS_TEXT_INDEX_PATH, LOCAL_TEXT_INDEX_PATH),
        (GCS_TEXT_LABELS_PATH, LOCAL_TEXT_LABELS_PATH),
        (GCS_ANOMALY_INDEX_PATH, LOCAL_ANOMALY_INDEX_PATH),
        (GCS_ANOMALY_LABELS_PATH, LOCAL_ANOMALY_LABELS_PATH),
        (GCS_DATASET_PATH, LOCAL_DATASET_PATH),
    ]
    for gcs_path, local_path in files_to_download:
        blob = bucket.blob(gcs_path)
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        blob.download_to_filename(local_path)
        logger.info(f"T·∫£i v·ªÅ {gcs_path} to {local_path}")

def upload_to_gcs(local_path: str, destination_blob_name: str):
    client = storage.Client.from_service_account_json("app/iamkey.json")
    bucket = client.bucket(GCS_BUCKET)
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(local_path)
    logger.info(f"ƒê√£ upload {local_path} l√™n GCS t·∫°i: gs://{GCS_BUCKET}/{destination_blob_name}")

def preprocess_image(image_data: bytes) -> Optional[np.ndarray]:
    try:
        nparr = np.frombuffer(image_data, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_GRAYSCALE)
        if img is None:
            return None
        blurred = cv2.GaussianBlur(img, (5, 5), 0)
        equalized = cv2.equalizeHist(blurred)
        edges = cv2.Canny(equalized, 50, 150)
        return edges
    except Exception as e:
        logger.error(f"L·ªói x·ª≠ l√Ω ·∫£nh: {e}")
        return None

def embed_image(image_data: bytes):
    try:
        image = Image.open(BytesIO(image_data)).convert("RGB")
        inputs = processor(images=image, return_tensors="pt").to(device)
        with torch.no_grad():
            embedding = model.get_image_features(**inputs)
        return embedding.cpu().numpy().astype(np.float32)
    except Exception as e:
        logger.error(f"L·ªói nh√∫ng ·∫£nh: {e}")
        return None

def generate_anomaly_map(image_data: bytes) -> Optional[np.ndarray]:
    try:
        img = Image.open(BytesIO(image_data)).convert("RGB")
        original_size = img.size
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
        ])
        img_tensor = transform(img).unsqueeze(0).to(device)
        with torch.no_grad():
            features = vit_model.forward_features(img_tensor)
        feature_map = features.mean(dim=1).squeeze().cpu().numpy()
        anomaly_map = (feature_map - np.min(feature_map)) / (np.ptp(feature_map) + 1e-6)
        anomaly_map = (anomaly_map * 255).astype(np.uint8)
        anomaly_map_resized = cv2.resize(anomaly_map, original_size, interpolation=cv2.INTER_CUBIC)
        return anomaly_map_resized
    except Exception as e:
        logger.error(f"L·ªói t·∫°o Anomaly Map: {e}")
        return None

def embed_anomaly_map(anomaly_map: np.ndarray):
    try:
        anomaly_map_rgb = cv2.cvtColor(anomaly_map, cv2.COLOR_GRAY2RGB)
        image = Image.fromarray(anomaly_map_rgb)
        inputs = processor(images=image, return_tensors="pt").to(device)
        with torch.no_grad():
            embedding = model.get_image_features(**inputs)
        return embedding.cpu().numpy().astype(np.float32)
    except Exception as e:
        logger.error(f"L·ªói nh√∫ng Anomaly Map: {e}")
        return None

def load_faiss_index():
    global index, labels, anomaly_index, anomaly_labels
    try:
        if os.path.exists(LOCAL_INDEX_PATH):
            index = faiss.read_index(LOCAL_INDEX_PATH)
            logger.info(f"FAISS Index t·∫£i th√†nh c√¥ng! T·ªïng s·ªë vector: {index.ntotal}")
        else:
            logger.warning("FAISS Index kh√¥ng t·ªìn t·∫°i!")
        if os.path.exists(LOCAL_TEXT_LABELS_PATH):
            labels = np.load(LOCAL_TEXT_LABELS_PATH, allow_pickle=True).tolist()
            logger.info(f"ƒê√£ t·∫£i {len(labels)} nh√£n b·ªánh t·ª´ labels.npy")
        else:
            logger.warning("labels.npy kh√¥ng t·ªìn t·∫°i!")
        if os.path.exists(LOCAL_ANOMALY_INDEX_PATH):
            anomaly_index = faiss.read_index(LOCAL_ANOMALY_INDEX_PATH)
            logger.info(f"FAISS Anomaly Index t·∫£i th√†nh c√¥ng! T·ªïng s·ªë vector: {anomaly_index.ntotal}")
        else:
            logger.warning("FAISS Anomaly Index kh√¥ng t·ªìn t·∫°i!")
        if os.path.exists(LOCAL_ANOMALY_LABELS_PATH):
            anomaly_labels = np.load(LOCAL_ANOMALY_LABELS_PATH, allow_pickle=True).tolist()
            logger.info(f"ƒê√£ t·∫£i {len(anomaly_labels)} nh√£n b·ªánh t·ª´ labels_anomaly.npy")
        else:
            logger.warning("labels_anomaly.npy kh√¥ng t·ªìn t·∫°i!")
    except Exception as e:
        logger.error(f"L·ªói t·∫£i FAISS Index: {e}")

def search_similar_images(query_vector, top_k=5):
    if index is None or index.ntotal == 0:
        logger.warning("FAISS index tr·ªëng!")
        return []
    try:
        if query_vector.ndim == 1:
            query_vector = np.expand_dims(query_vector, axis=0)
        faiss.normalize_L2(query_vector)

        distances, indices = index.search(query_vector, top_k)
        logger.info(f"Ch·ªâ s·ªë t√¨m th·∫•y: {indices}")
        logger.info(f"Cosine similarities: {distances}")

        similar_results = []
        for idx, sim in zip(indices[0], distances[0]):
            if sim < 80:  # Lo·∫°i b·ªè nh√£n c√≥ similarity d∆∞·ªõi 0.8
                continue
            if 0 <= idx < len(labels):
                label_filename = list(labels.keys())[idx]
                label = labels[label_filename]
            else:
                logger.warning(f"Index {idx} v∆∞·ª£t ph·∫°m vi labels ({len(labels)})!")
                label = "unknown"
            similar_results.append({
                "label": label,
                "cosine_similarity": float(sim)
            })
        return similar_results
    except Exception as e:
        logger.error(f"L·ªói t√¨m ki·∫øm ·∫£nh t∆∞∆°ng t·ª±: {e}")
        return []

def search_anomaly_images(query_vector, top_k=5):
    if anomaly_index is None or anomaly_index.ntotal == 0:
        logger.warning("FAISS Anomaly Index tr·ªëng!")
        return []
    try:
        if query_vector.ndim == 1:
            query_vector = np.expand_dims(query_vector, axis=0)
        faiss.normalize_L2(query_vector)

        distances, indices = anomaly_index.search(query_vector, top_k)
        logger.info(f"Ch·ªâ s·ªë t√¨m th·∫•y: {indices}")
        logger.info(f"Cosine similarities (anomaly): {distances}")

        similar_results = []
        for idx, sim in zip(indices[0], distances[0]):
            if sim < 80:  # Lo·∫°i b·ªè nh√£n c√≥ similarity d∆∞·ªõi 0.8
                continue    
            if 0 <= idx < len(anomaly_labels):
                label_filename = list(anomaly_labels.keys())[idx]
                label = anomaly_labels[label_filename]
            else:
                logger.warning(f"Index {idx} v∆∞·ª£t ph·∫°m vi labels_anomaly ({len(anomaly_labels)})!")
                label = "unknown"
            similar_results.append({
                "label": label,
                "cosine_similarity": float(sim)
            })
        return similar_results
    except Exception as e:
        logger.error(f"L·ªói t√¨m ki·∫øm ·∫£nh anomaly: {e}")
        return []

def combine_labels(detailed_labels_normal: List[Dict], detailed_labels_anomaly: List[Dict]) -> str:
    """
    K·∫øt h·ª£p nh√£n t·ª´ detailed_labels_normal v√† detailed_labels_anomaly, lo·∫°i b·ªè tr√πng l·∫∑p v√† nh√£n c√≥ similarity < 0.8.
    Args:
        detailed_labels_normal: Danh s√°ch dict ch·ª©a label v√† cosine_similarity t·ª´ ·∫£nh g·ªëc.
        detailed_labels_anomaly: Danh s√°ch dict ch·ª©a label v√† cosine_similarity t·ª´ anomaly map.
    Returns:
        Chu·ªói c√°c nh√£n ƒë∆∞·ª£c k·∫øt h·ª£p, s·∫Øp x·∫øp theo cosine_similarity gi·∫£m d·∫ßn.
    """
    # K·∫øt h·ª£p t·∫•t c·∫£ nh√£n t·ª´ c·∫£ hai ngu·ªìn
    all_labels = detailed_labels_normal + detailed_labels_anomaly
    
    # Chu·∫©n h√≥a v√† l·ªçc nh√£n
    filtered_labels = []
    seen_labels = {}  # L∆∞u nh√£n v√† similarity cao nh·∫•t
    for item in all_labels:
        label = item["label"]
        sim = item["cosine_similarity"]
        # Chu·∫©n h√≥a similarity n·∫øu ·ªü thang 0-100
        normalized_sim = sim / 100.0 if sim > 1.0 else sim
        if normalized_sim < 0.8:  # Lo·∫°i b·ªè nh√£n c√≥ similarity < 0.8
            continue
        # Gi·ªØ nh√£n c√≥ similarity cao nh·∫•t n·∫øu tr√πng l·∫∑p
        if label not in seen_labels or normalized_sim > seen_labels[label]:
            seen_labels[label] = normalized_sim
    
    # T·∫°o danh s√°ch nh√£n ƒë√£ l·ªçc v√† s·∫Øp x·∫øp theo similarity gi·∫£m d·∫ßn
    filtered_labels = [
        {"label": label, "cosine_similarity": sim}
        for label, sim in seen_labels.items()
    ]
    filtered_labels.sort(key=lambda x: x["cosine_similarity"], reverse=True)
    
    # T·∫°o chu·ªói nh√£n
    final_labels = " ".join(item["label"] for item in filtered_labels).strip()
    
    logger.info(f"Nh√£n t·ªïng h·ª£p sau l·ªçc v√† s·∫Øp x·∫øp: {final_labels}")
    return final_labels

def generate_description_with_Gemini(image_data: bytes):
    try:
        img = Image.open(BytesIO(image_data))
        model = genai.GenerativeModel('gemini-2.5-flash')
        prompt = """
        M√¥ t·∫£ b·ª©c ·∫£nh n√†y b·∫±ng ti·∫øng Vi·ªát, ƒë√¢y l√† ·∫£nh y khoa n√™n h√£y m√¥ t·∫£ th·∫≠t k·ªπ.
        Ch·ªâ t·∫≠p trung v√†o m√¥ t·∫£ l√¢m s√†ng, kh√¥ng ƒë∆∞a ra ch·∫©n ƒëo√°n hay k·∫øt lu·∫≠n.
        H√£y m√¥ t·∫£ c√°c ƒë·∫∑c ƒëi·ªÉm sau:
        - V·ªã tr√≠ c·ªßa t·ªïn th∆∞∆°ng (v√≠ d·ª•: l√≤ng b√†n tay, mu b√†n tay, ng√≥n ch√¢n...)
        - K√≠ch th∆∞·ªõc t·ªïn th∆∞∆°ng (∆∞·ªõc l∆∞·ª£ng theo mm ho·∫∑c cm)
        - M√†u s·∫Øc (ƒë·ªìng nh·∫•t hay nhi·ªÅu m√†u, ƒë·ªè, t√≠m, h·ªìng, v.v.)
        - K·∫øt c·∫•u b·ªÅ m·∫∑t da (m·ªãn, s·∫ßn s√πi, c√≥ v·∫£y, lo√©t...)
        - ƒê·ªô r√µ n√©t c·ªßa c√°c c·∫°nh t·ªïn th∆∞∆°ng (r√µ r√†ng hay m·ªù, lan t·ªèa)
        - T√≠nh ƒë·ªëi x·ª©ng (t·ªïn th∆∞∆°ng c√≥ ƒë·ªëi x·ª©ng 2 b√™n hay kh√¥ng)
        - Ph√¢n b·ªë (r·∫£i r√°c, t·∫≠p trung th√†nh ƒë√°m, theo ƒë∆∞·ªùng‚Ä¶)
        - C√°c ƒë·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng kh√°c n·∫øu c√≥ (ch·∫£y m√°u, v·∫£y, m·ª•n n∆∞·ªõc, s∆∞ng n·ªÅ‚Ä¶)
        Ch·ªâ m√¥ t·∫£ nh·ªØng g√¨ c√≥ th·ªÉ nh√¨n th·∫•y trong ·∫£nh, kh√¥ng ƒë∆∞a ra gi·∫£ ƒë·ªãnh hay ch·∫©n ƒëo√°n y khoa.
        X√≥a mark down v√† c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát trong k·∫øt qu·∫£.
        """
        response = model.generate_content([prompt, img])
        caption = response.text.replace("\n", " ")
        logger.info("T·∫°o m√¥ t·∫£ ·∫£nh th√†nh c√¥ng v·ªõi Gemini")
        return caption
    except Exception as e:
        logger.error(f"L·ªói khi t·∫°o caption v·ªõi Gemini: {e}")
        return None

def generate_medical_entities(image_caption, user_description):
    combined_description = f"1. M√¥ t·∫£ t·ª´ ng∆∞·ªùi d√πng: {user_description}. 2. M√¥ t·∫£ t·ª´ ·∫£nh: {image_caption}."
    print(combined_description)

    prompt = textwrap.dedent(f"""
        T√¥i c√≥ 2 ƒëo·∫°n m√¥ t·∫£ sau v·ªÅ m·ªôt v√πng da b·ªã b·∫•t th∆∞·ªùng: {combined_description}
        H√£y chu·∫©n h√≥a c·∫£ hai m√¥ t·∫£, lo·∫°i b·ªè t·ª´ d∆∞ th·ª´a, h·ª£p nh·∫•t l·∫°i, v√† tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng y khoa quan tr·ªçng.
        M·ªói ƒë·∫∑c tr∆∞ng n√™n ƒë∆∞·ª£c g·∫Øn nh√£n thu·ªôc m·ªôt trong ba lo·∫°i sau:
        - "Tri·ªáu ch·ª©ng": m√¥ t·∫£ bi·ªÉu hi·ªán, d·∫•u hi·ªáu l√¢m s√†ng (v√≠ d·ª•: ph√°t ban, ng·ª©a, ƒë·ªè, bong tr√≥c‚Ä¶)
        - "V·ªã tr√≠ xu·∫•t hi·ªán": v√πng c∆° th·ªÉ b·ªã ·∫£nh h∆∞·ªüng (v√≠ d·ª•: mu b√†n tay, c·∫≥ng ch√¢n, ng√≥n tay‚Ä¶)
        - "Nguy√™n nh√¢n": y·∫øu t·ªë g√¢y ra t√¨nh tr·∫°ng ƒë√≥ n·∫øu c√≥ xu·∫•t hi·ªán trong m√¥ t·∫£ (v√≠ d·ª•: c√¥n tr√πng c·∫Øn, d·ªã ·ª©ng, ti·∫øp x√∫c h√≥a ch·∫•t‚Ä¶)
        Tr·∫£ v·ªÅ k·∫øt qu·∫£ d·∫°ng JSON Array. M·ªói ph·∫ßn t·ª≠ l√† m·ªôt object g·ªìm:
        - "entity": c·ª•m t·ª´ y khoa
        - "type": "Tri·ªáu ch·ª©ng", "V·ªã tr√≠ xu·∫•t hi·ªán", ho·∫∑c "Nguy√™n nh√¢n"
        V√≠ d·ª• ƒë·∫ßu ra:
        [
          {{ "entity": "v·∫øt ƒë·ªè", "type": "Tri·ªáu ch·ª©ng" }},
          {{ "entity": "c·∫≥ng ch√¢n", "type": "V·ªã tr√≠ xu·∫•t hi·ªán" }},
          {{ "entity": "d·ªã ·ª©ng th·ªùi ti·∫øt", "type": "Nguy√™n nh√¢n" }}
        ]
        Ch·ªâ li·ªát k√™ c√°c ƒë·∫∑c tr∆∞ng c√≥ trong m√¥ t·∫£. Kh√¥ng suy lu·∫≠n th√™m.
    """)
    try:
        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content([prompt])
        text = response.text.strip()
        clean_text = re.sub(r"```(?:json)?|```", "", text).strip()
        result = json.loads(clean_text)
        decoded_result = json.dumps(result, ensure_ascii=False)
        return decoded_result 

    except Exception as e:
        print(f"L·ªói khi t·∫°o m√¥ t·∫£ v·ªõi Gemini: {e}")
        return None

def compare_descriptions_and_labels(description: str, labels: str):
    prompt = textwrap.dedent(f"""
        M√¥ t·∫£: "{description}"
        Nh√£n: "{labels}"
        So s√°nh s·ª± kh√°c bi·ªát gi·ªØa m√¥ t·∫£ v√† nh√£n b·ªánh. Sau ƒë√≥, t·∫°o ra 3 c√¢u h·ªèi gi√∫p ph√¢n bi·ªát ch√≠nh x√°c h∆°n.
        Tr·∫£ v·ªÅ k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng:
        1. ...
        2. ...
        3. ...
    """)
    try:
        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content([prompt])
        text = response.text.strip()
        questions = re.findall(r"\d+\.\s+(.*)", text)
        logger.info("T·∫°o c√¢u h·ªèi ph√¢n bi·ªát th√†nh c√¥ng")
        return questions
    except Exception as e:
        logger.error(f"L·ªói khi g·ªçi Gemini: {e}")
        return []

def filter_incorrect_labels_by_user_description(description: str, labels: list[str]) -> str:
    prompt = textwrap.dedent(f"""
        M√¥ t·∫£ b·ªánh c·ªßa ng∆∞·ªùi d√πng: "{description}"
        Danh s√°ch c√°c nh√£n b·ªánh nghi ng·ªù: [{labels}]

        Nhi·ªám v·ª•:
        1. Ph√¢n t√≠ch m√¥ t·∫£ v√† so s√°nh v·ªõi t·ª´ng nh√£n b·ªánh.
        2. Lo·∫°i b·ªè c√°c nh√£n b·ªánh kh√¥ng ph√π h·ª£p v·ªõi m√¥ t·∫£. Gi·∫£i th√≠ch l√Ω do lo·∫°i b·ªè r√µ r√†ng.
        3. Gi·ªØ l·∫°i c√°c nh√£n ph√π h·ª£p nh·∫•t, s·∫Øp x·∫øp theo m·ª©c ƒë·ªô ph√π h·ª£p gi·∫£m d·∫ßn.
        4. Tr·∫£ th√™m similarity c·ªßa t·ª´ng nh√£n b·ªánh 
        K·∫øt qu·∫£ ƒë·∫ßu ra ph·∫£i ·ªü ƒë·ªãnh d·∫°ng JSON:
        {{
            "loai_bo": [{{"label": "nh√£n kh√¥ng ph√π h·ª£p", "ly_do": "...","similarity":""}}],
            "giu_lai": [{{"label": "nh√£n ph√π h·ª£p", "do_phu_hop": "cao/trung b√¨nh/th·∫•p", "similarity":""}}]
        }}
    """)

    try:
        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content([prompt])
        text = response.text.strip()
        clean_text = re.sub(r"```(?:json)?|```", "", text).strip()
        result = json.loads(clean_text)
        return result 

    except Exception as e:
        print(f"L·ªói khi t·∫°o m√¥ t·∫£ v·ªõi Gemini: {e}")
        return None

def search_disease_in_json(file_path: str, disease_name: str) -> List[Dict]:
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if not isinstance(data, list):
            logger.warning("File JSON kh√¥ng ph·∫£i l√† danh s√°ch.")
            return []
        results = [
            entry for entry in data
            if isinstance(entry, dict) and disease_name.lower() in entry.get("T√™n b·ªánh", "").lower()
        ]
        logger.info(f"T√¨m th·∫•y {len(results)} k·∫øt qu·∫£ trong JSON cho b·ªánh: {disease_name}")
        return results
    except Exception as e:
        logger.error(f"L·ªói t√¨m ki·∫øm trong JSON: {e}")
        return []
    
def append_disease_to_json(file_path: str, new_disease: dict):
    with open(file_path, 'r+', encoding='utf-8') as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError:
            data = []
        data.append(new_disease)
        f.seek(0)
        json.dump(data, f, ensure_ascii=False, indent=2)
        f.truncate()
    print(f"Added new disease: {new_disease.get('name', '')}")

def upload_json_to_gcs(bucket_name: str, destination_blob_name: str, source_file_name: str):
    client = storage.Client.from_service_account_json("app/iamkey            .json")
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(source_file_name)
    print(f"Uploaded {source_file_name} to {destination_blob_name}.")
    

def search_final(name):
    translate_name=translate_disease_name(name)
    print(f"T√™n b·ªánh ƒë√£ d·ªãch: {translate_name}")
    search_json_result = search_disease_in_json(LOCAL_DATASET_PATH, translate_name)
    if search_json_result:
        print(f"K·∫øt qu·∫£ t√¨m ki·∫øm trong file JSON: {search_json_result}")
    else:
        print(f"Kh√¥ng t√¨m th·∫•y t√™n b·ªánh '{translate_name}' trong file JSON.")
        print ("B·∫Øt ƒë·∫ßu t√¨m ki·∫øm b·∫±ng MedlinePlus...")
        search_medline_result = search_medlineplus(name)
        print(f"K·∫øt qu·∫£ t√¨m ki·∫øm MedlinePlus: {search_medline_result}")
        print ("B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t th√¥ng tin y khoa t·ª´ MedlinePlus...")
        extract_medical_info_result = extract_medical_info(search_medline_result)
        if extract_medical_info_result:
            print(f"K·∫øt qu·∫£ tr√≠ch xu·∫•t th√¥ng tin y khoa: {extract_medical_info_result}")
            print("ƒêang th√™m th√¥ng tin v√†o file JSON...")
            append_disease_to_json(LOCAL_DATASET_PATH, extract_medical_info_result)
            print("Upload file JSON l√™n GCS...")
            upload_json_to_gcs(GCS_BUCKET, GCS_DATASET_PATH, LOCAL_DATASET_PATH)

def translate_disease_name(disease_name: str) -> str:
    prompt = f"""
    B·∫°n l√† m·ªôt chuy√™n gia y t·∫ø, b·∫°n c√≥ kh·∫£ nƒÉng d·ªãch t√™n b·ªánh t·ª´ ti·∫øng Anh sang ti·∫øng Vi·ªát.
    T√™n b·ªánh ƒë∆∞·ª£c truy·ªÅn v√†o l√†: {disease_name}
    H√£y d·ªãch t√™n b·ªánh ƒë√≥ sang ti·∫øng Vi·ªát.
    Tr·∫£ v·ªÅ t√™n b·ªánh ƒë√£ d·ªãch.
    """
    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
        response = model.generate_content(prompt)
        result = response.text.strip()
        result = re.sub(r"^(?:\w+)?\n|\n$", "", result).strip()
        if not result:
            return "Kh√¥ng th·ªÉ d·ªãch t√™n b·ªánh"
        logger.info(f"D·ªãch t√™n b·ªánh: {disease_name} -> {result}")
        return result
    except Exception as e:
        logger.error(f"L·ªói khi d·ªãch t√™n b·ªánh: {e}")
        return "X·∫£y ra l·ªói trong qu√° tr√¨nh d·ªãch t√™n b·ªánh"

def search_medlineplus(ten_khoa_hoc: str) -> Optional[str]:
    if not ten_khoa_hoc or ten_khoa_hoc == "Kh√¥ng t√¨m th·∫•y":
        logger.warning("Kh√¥ng c√≥ t√™n b·ªánh truy·ªÅn v√†o cho MedlinePlus")
        return None
    logger.info(f"T√¨m ki·∫øm th√¥ng tin b·ªánh '{ten_khoa_hoc}' tr√™n MedlinePlus...")
    url = 'https://wsearch.nlm.nih.gov/ws/query'
    params = {'db': 'healthTopics', 'term': ten_khoa_hoc}
    try:
        response = requests.get(url, params=params)
        if response.status_code == 200:
            cleaned_content = clean_xml_content(response.content)
            logger.info("T√¨m ki·∫øm MedlinePlus th√†nh c√¥ng")
            return cleaned_content
        logger.warning(f"Y√™u c·∫ßu MedlinePlus th·∫•t b·∫°i, m√£ tr·∫°ng th√°i: {response.status_code}")
        return None
    except Exception as e:
        logger.error(f"L·ªói t√¨m ki·∫øm MedlinePlus: {e}")
        return None

def clean_xml_content(xml_content: bytes) -> str:
    try:
        root = ET.fromstring(xml_content)
        text_nodes = [elem.text.strip() for elem in root.iter() if elem.text and elem.text.strip()]
        return ' '.join(text_nodes)
    except ET.ParseError as e:
        logger.error(f"L·ªói ph√¢n t√≠ch XML: {e}")
        return ""
    
def combine_user_questions_and_answers(user_questions, user_answers):
    if not user_questions or not user_answers:
        logger.warning("Kh√¥ng c√≥ c√¢u h·ªèi ho·∫∑c c√¢u tr·∫£ l·ªùi t·ª´ ng∆∞·ªùi d√πng")
        return []
    prompt = f"""H√£y t·ªïng h·ª£p c√°c c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi c·ªßa ng∆∞·ªùi d√πng theo th·ª© t·ª± 
    Danh s√°ch c√¢u h·ªèi{ user_questions}
    Danh s√°ch c√¢u tr·∫£ l·ªùi{ user_answers}
    V√≠ d·ª•:B·∫°n c√≥ n·ªïi m·∫´n ƒë√≥ kh√¥ng? C√≥.
    Tr·∫£ v·ªÅ d·∫°ng chu·ªói c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi, m·ªói c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi c√°ch nhau b·∫±ng d·∫•u ph·∫©y.
    """
    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
        response = model.generate_content(prompt)
        result = response.text.strip()
        if not result:
            logger.warning("Kh√¥ng c√≥ k·∫øt qu·∫£ tr·∫£ v·ªÅ t·ª´ Gemini")
            return []
        logger.info("T·ªïng h·ª£p c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi th√†nh c√¥ng")
        return result.split(", ")
    except Exception as e:
        logger.error(f"L·ªói khi t·ªïng h·ª£p c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi: {e}")
        return []

def extract_medical_info(text: str) -> Dict:
    prompt = f"""
    D·ªãch vƒÉn b·∫£n v·ªÅ ti·∫øng Vi·ªát
    B·∫°n l√† m·ªôt chuy√™n gia y t·∫ø, b·∫°n c√≥ kh·∫£ nƒÉng tr√≠ch xu·∫•t th√¥ng tin y khoa t·ª´ vƒÉn b·∫£n.
    H√£y tr√≠ch xu·∫•t th√¥ng tin y khoa t·ª´ vƒÉn b·∫£n d∆∞·ªõi d·∫°ng JSON h·ª£p l·ªá **kh√¥ng ch·ª©a Markdown**.
    Ch·ªâ l·∫•y k·∫øt qu·∫£ ƒë·∫ßu ti√™n m√† b·∫°n t√¨m ƒë∆∞·ª£c
    H√£y tr√≠ch xu·∫•t th·∫≠t chi ti·∫øt
    VƒÉn b·∫£n ƒë·∫ßu v√†o l√†:
    {text}
    {{
        "T√™n b·ªánh": "",
        "T√™n khoa h·ªçc": "",
        "Tri·ªáu ch·ª©ng": "",
        "V·ªã tr√≠ xu·∫•t hi·ªán": "",
        "Nguy√™n nh√¢n": "",
        "Ti√™u ch√≠ ch·∫©n ƒëo√°n": "",
        "Ch·∫©n ƒëo√°n ph√¢n bi·ªát": "",
        "ƒêi·ªÅu tr·ªã": "",
        "Ph√≤ng b·ªánh": "",
        "C√°c lo·∫°i thu·ªëc": [{{"T√™n thu·ªëc": "", "Li·ªÅu l∆∞·ª£ng": "", "Th·ªùi gian s·ª≠ d·ª•ng": ""}}]
    }}
    - N·∫øu kh√¥ng c√≥ th√¥ng tin, ƒë·∫∑t gi√° tr·ªã "Kh√¥ng t√¨m th·∫•y".
    - Kh√¥ng th√™m gi·∫£i th√≠ch, kh√¥ng in Markdown, kh√¥ng th√™m k√Ω t·ª± th·ª´a.
    """
    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
        response = model.generate_content(prompt)
        raw_text = response.text if hasattr(response, "text") else response.parts[0].text
        raw_text = re.sub(r"^```json\n|\n```$", "", raw_text)
        extracted_info = json.loads(raw_text)
        cleaned_info = clean_text_json(extracted_info)
        logger.info("Tr√≠ch xu·∫•t th√¥ng tin y khoa t·ª´ MedlinePlus th√†nh c√¥ng")
        return cleaned_info
    except Exception as e:
        logger.error(f"L·ªói tr√≠ch xu·∫•t th√¥ng tin y khoa: {e}")
        return {}

def clean_text_json(data):
    if isinstance(data, dict):
        return {key: clean_text_json(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [clean_text_json(item) for item in data]
    else:
        return clean_text(str(data))

def clean_text(text: str) -> str:
    if not isinstance(text, str):
        return str(text)
    text = re.sub(r'[\\\n\r\t\*\]]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def process_image(image_data: bytes):
    processed = preprocess_image(image_data)
    if processed is None:
        logger.error("L·ªói x·ª≠ l√Ω ·∫£nh, d·ª´ng quy tr√¨nh.")
        return None, [], [], [], []

    processed_dir = Path("app/static/processed")
    processed_dir.mkdir(parents=True, exist_ok=True)
    processed_path = processed_dir / "processed_temp.jpg"
    cv2.imwrite(str(processed_path), processed)
    upload_to_gcs(str(processed_path), GCS_IMAGE_PATH + str(processed_path.name))

    embedding = embed_image(image_data)
    result_labels_simple = []
    detailed_labels_normal = []
    if embedding is not None:
        detailed_labels_normal = search_similar_images(embedding)
        result_labels_simple = [item["label"] for item in detailed_labels_normal]
        logger.info("üîç ·∫¢nh g·ªëc:")
        for item in detailed_labels_normal:
            logger.info(f"- {item['label']} (cosine: {item['cosine_similarity']:.4f})")

    anomaly_map = generate_anomaly_map(image_data)
    anomaly_result_labels_simple = []
    detailed_labels_anomaly = []
    if anomaly_map is not None:
        anomaly_map_path = processed_dir / "anomaly_map_temp.jpg"
        cv2.imwrite(str(anomaly_map_path), anomaly_map)
        upload_to_gcs(str(anomaly_map_path), GCS_IMAGE_PATH + str(anomaly_map_path.name))

        anomaly_map_embedding = embed_anomaly_map(anomaly_map)
        if anomaly_map_embedding is not None:
            detailed_labels_anomaly = search_anomaly_images(anomaly_map_embedding)
            anomaly_result_labels_simple = [item["label"] for item in detailed_labels_anomaly]
            logger.info("Anomaly Map:")
            for item in detailed_labels_anomaly:
                logger.info(f"- {item['label']} (cosine: {item['cosine_similarity']:.4f})")
    final_labels = combine_labels(detailed_labels_anomaly, detailed_labels_normal)
    logger.info(f"Nh√£n t·ªïng h·ª£p cu·ªëi c√πng: {final_labels}")

    return final_labels, result_labels_simple, anomaly_result_labels_simple, detailed_labels_normal, detailed_labels_anomaly
async def start_diagnois(image: UploadFile = File(...),user_id: Optional[str] = None):
    try:
        download_from_gcs()
        load_faiss_index()
        if not image.filename.lower().endswith(('.jpg', '.jpeg', '.png')):
            raise HTTPException(status_code=400, detail="·∫¢nh ph·∫£i c√≥ ƒë·ªãnh d·∫°ng .jpg, .jpeg ho·∫∑c .png")
        image_data = await image.read()

        Key =user_id
        print(f"Key ng∆∞·ªùi d√πng: {Key}")
        final_labels, result_labels_simple, anomaly_result_labels_simple, detailed_labels_normal, detailed_labels_anomaly = process_image(image_data)
        if not final_labels:
            raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ x·ª≠ l√Ω ·∫£nh")

        image_description = generate_description_with_Gemini(image_data)
        if not image_description:
            raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ t·∫°o m√¥ t·∫£ ·∫£nh")

        logger.info(f"M√¥ t·∫£ ·∫£nh: {image_description}")

        result_data = {
            "final_labels": final_labels,
            "image_description": image_description,
            "result_labels": result_labels_simple,
            "anomaly_result_labels": anomaly_result_labels_simple,
            "detailed_labels_normal": detailed_labels_normal,
            "detailed_labels_anomaly": detailed_labels_anomaly
        }
        saved = await save_result_to_redis(key=Key, value=result_data)
        if not saved:
            logger.warning("Kh√¥ng th·ªÉ l∆∞u k·∫øt qu·∫£ v√†o Redis")
            raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ l∆∞u k·∫øt qu·∫£ v√†o Redis")

        return JSONResponse(content=result_data, status_code=200)

    except HTTPException as e:
        logger.error(f"L·ªói HTTP: {e.detail}")
        raise e
    except Exception as e:
        logger.error(f"L·ªói kh√°c: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói trong qu√° tr√¨nh ch·∫©n ƒëo√°n: {str(e)}")
    
async def get_diagnosis_result(key: str):
    result = await get_result_by_key(key)
    if not result:
        raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£")
    return result

async def submit_user_description(user_description: str, key: str):
    try:
        if not user_description or not key:
            raise HTTPException(status_code=400, detail="Thi·∫øu m√¥ t·∫£ ·∫£nh ho·∫∑c key")
        result = await get_result_by_key(key)
        if not result:
            raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ch·∫©n ƒëo√°n")
        current_data_json = await get_diagnosis_result(key)
        if not current_data_json:
            raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ch·∫©n ƒëo√°n hi·ªán t·∫°i")
        current_data = current_data_json
        current_data["user_description"] = user_description

        await redis_client.set(key, json.dumps(current_data), ex=3600)  
        return JSONResponse(content={"message": "M√¥ t·∫£ ng∆∞·ªùi d√πng ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng"}, status_code=200)
    except HTTPException as e:
        logger.error(f"L·ªói khi l∆∞u m√¥ t·∫£ ng∆∞·ªùi d√πng: {e}")
        raise HTTPException(status_code=500, detail=f"L·ªói khi l∆∞u m√¥ t·∫£ ng∆∞·ªùi d√πng: {str(e)}")     
        
async def get_differentiation_questions(key:str):
    try:
        result = await get_result_by_key(key)
        if not result:
            raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ch·∫©n ƒëo√°n")
        user_description = result.get("user_description", "")
        if not user_description or not key:
            raise HTTPException(status_code=400, detail="Thi·∫øu m√¥ t·∫£ ·∫£nh ho·∫∑c key")
        result_medical_entities = generate_medical_entities(
            user_description or "Kh√¥ng c√≥ m√¥ t·∫£ t·ª´ ng∆∞·ªùi d√πng",
            result.get("image_description", "") or "Kh√¥ng c√≥ m√¥ t·∫£ t·ª´ ·∫£nh"
        )
        if not result_medical_entities:
            raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ t·∫°o m√¥ t·∫£ y khoa")
        
        questions = compare_descriptions_and_labels(result_medical_entities,result.get("final_labels", ""))
        if not questions:
            raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ t·∫°o c√¢u h·ªèi ph√¢n bi·ªát")
        logger.info(f"C√¢u h·ªèi ph√¢n bi·ªát: {questions}")
        current_data_json = await get_diagnosis_result(key)
        if not current_data_json:
            raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ch·∫©n ƒëo√°n hi·ªán t·∫°i")
        current_data= current_data_json
        current_data["questions"] = questions
        current_data["medical_entities"] = result_medical_entities

        await redis_client.set(key, json.dumps(current_data), ex=3600)  
        return JSONResponse(content=[{"questions": questions},{"medical_entites":result_medical_entities}], status_code=200)
    except HTTPException as e:
        logger.error(f"L·ªói khi t·∫°o c√¢u h·ªèi ph√¢n bi·ªát: {e}")
        raise HTTPException(status_code=500, detail=f"L·ªói khi t·∫°o c√¢u h·ªèi ph√¢n bi·ªát: {str(e)}")

async def submit_differation_questions(user_answers:dict,key:str):
    try:
        result = await get_result_by_key(key)
        if not result:
            raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ch·∫©n ƒëo√°n")
        medical_entities = result.get("medical_entities", "")
        if not medical_entities:
            raise HTTPException(status_code=500, detail="Kh√¥ng c√≥ m√¥ t·∫£ y khoa")
        if not user_answers or not isinstance(user_answers, dict):
            raise HTTPException(status_code=400, detail="D·ªØ li·ªáu c√¢u tr·∫£ l·ªùi kh√¥ng h·ª£p l·ªá")
        questions = result.get("questions", [])
        if not questions:
            raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y c√¢u h·ªèi ph√¢n bi·ªát")
        combined_answer = combine_user_questions_and_answers(questions, user_answers)

        combined_description= f"{medical_entities}\n\n{combined_answer}"
        final_labels = result.get("final_labels", "")
        print("\n--- ƒêang lo·∫°i tr·ª´ nh√£n kh√¥ng ph√π h·ª£p ---")
        result_filter =filter_incorrect_labels_by_user_description(combined_description, final_labels)
        if not result_filter:
            print("Kh√¥ng c√≥ k·∫øt qu·∫£ t·ª´ Gemini.")
            return
        refined_labels = result_filter.get("giu_lai", [])
        if not refined_labels:
            print("Kh√¥ng c√≤n nh√£n n√†o ph√π h·ª£p. ƒê·ªÅ xu·∫•t tham kh·∫£o b√°c sƒ©.")
        else:
            print("C√°c nh√£n c√≤n l·∫°i sau lo·∫°i tr·ª´:")
        
        result_redis = []
        for label_info in refined_labels:
            label = label_info.get("label")
            ket_qua = "-".join(label.split("-")[1:])
            suitability = label_info.get("do_phu_hop")
            similarity= label_info.get("similarity", "")
            print(f"- {ket_qua} (M·ª©c ƒë·ªô ph√π h·ª£p: {suitability}) (Similarity: {similarity})")
            result_redis.append({"ketqua": ket_qua,"do_phu_hop": suitability, "similarity": similarity})
        current_data_json = await get_diagnosis_result(key)
        current_data_json["result"] = result_redis
        print(result_redis)
        await redis_client.set(key, json.dumps(current_data_json), ex=3600) 
        if not current_data_json:
            raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ch·∫©n ƒëo√°n hi·ªán t·∫°i")
        return JSONResponse(content=[{"result":result_redis}], status_code=200)
    except HTTPException as e:
        logger.error(f"L·ªói khi lo·∫°i tr·ª´ nh√£n kh√¥ng ph√π h·ª£p: {e.detail}")
        raise HTTPException(status_code=500, detail=f"L·ªói khi lo·∫°i tr·ª´ nh√£n kh√¥ng ph√π h·ª£p: {str(e)}")
        
async def knowledge(disease_name: str):
    try:
        if not disease_name:
            raise HTTPException(status_code=400, detail="C·∫ßn cung c·∫•p t√™n b·ªánh")
        translated_name = translate_disease_name(disease_name)
        search_result = search_disease_in_json(LOCAL_DATASET_PATH, translated_name)
        if not search_result:
            medline_result = search_medlineplus(disease_name)
            if medline_result:
                extracted_info = extract_medical_info(medline_result)
                if extracted_info:
                    search_result = [extracted_info]
        if not search_result:
            raise HTTPException(status_code=404, detail=f"Kh√¥ng t√¨m th·∫•y th√¥ng tin cho b·ªánh: {disease_name}")
        logger.info(f"Tra c·ª©u th√¥ng tin b·ªánh: {disease_name}")
        return JSONResponse(content={"disease_info": search_result}, status_code=200)
    except Exception as e:
        logger.error(f"L·ªói khi tra c·ª©u th√¥ng tin b·ªánh: {e}")
        raise HTTPException(status_code=500, detail=f"L·ªói khi tra c·ª©u th√¥ng tin b·ªánh: {str(e)}")

async def get_final_result(key: str):
    try:
        result = await get_result_by_key(key)
        result_diagnosis = result.get("result", [])
        if not result:
            raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ch·∫©n ƒëo√°n")
        if not result_diagnosis:
            raise HTTPException (status_code=404,detail="K·∫øt qu·∫£ kh√¥ng t·ªìn t·∫°i")
        return JSONResponse(content={ "diagnosis": result_diagnosis}, status_code=200)
    except Exception as e:
        logger.error(f"L·ªói khi l·∫•y k·∫øt qu·∫£ ch·∫©n ƒëo√°n: {e}")
        raise HTTPException(status_code=500, detail=f"L·ªói khi l·∫•y k·∫øt qu·∫£ ch·∫©n ƒëo√°n")